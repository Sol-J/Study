---
description: T5 (2019)
---

# Everything is text-to-text

 T5는 한 모델로 모든 문제를 풀면서도 모델의 규모를 역대급으로 키우겠다는 사고의 집약체라고 볼 수 있다. NLU 벤치마크인 GLUE와 SQuAD 등에 포함된 다양한 자연어이해 과제를 사전 훈련했다. 여기에 사용한 정제 텍스트 데이터만 700GB\(RoBERTa의 4.4배\), 모델 매개변수는 110억개\(RoBERTa의 32배\)에 달하는 규모이다. 빈칸 하나의 예측값이 다른 빈칸 예측에 영향을 주지 않는 BERT의 한계를 극복하고자 seq2seq 구조의 MLM을 적용했다. 그 결과, T5는 SuperGLUE에서 인간과 비슷한 성능을 달성했다.

