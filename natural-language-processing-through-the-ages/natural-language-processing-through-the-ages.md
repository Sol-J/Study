---
description: 'Reference: NLP for Supervised Learning - A Brief Survey (Eugene Yan)'
---

# Intro

 아래의 포스트는 시대를 거쳐 자연어처리가 어떻게 진화되어 왔는지 정리해주고 있다. 본문에서 제시하고 있는 흐름에 따라 자연어처리 방법론들이 출현한 배경을 살펴보고자 한다. 

{% embed url="https://towardsdatascience.com/nlp-supervised-learning-survey-f5193c4120dc" %}

   • Sequential models:  RNN \(1985\), LSTM \(1997\), GRU \(2014\)

   • Word embeddings:  Word2vec \(2013\), GloVe \(2014\), FastText \(2016\)

   • Word embeddings with context:  ELMo \(2018\)

   • Attention:  Transformer \(2017\)

   • Pre-training:  ULMFiT \(2017\), GPT \(2017\)

   • Combining the above:  BERT \(2018\)

   • Improving BERT:  DistilBERT, ALBERT, RoBERTa, XLNet, Big Bird, Multilingual embeddings

   • Everything is text-to-text:  T5 \(2019\)



