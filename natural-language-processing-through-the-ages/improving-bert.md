---
description: >-
  DistilBERT, ALBERT, RoBERTa, XLNet (2019); Big Bird, Multilingual embeddings
  (2020)
---

# Improving BERT

 최근에는 MLM 방식에 적응한 BERT가 일부 문장이 비어있지 않은 단어 시퀀스 데이터 훈련에 제대로 적응하지 못한다는 한계가 지적되고 있고, 또한 여러 단어를 동시에 예측할 때 해당 단어 간 상관관계를 고려하지 않는 점도 지적되고 있다. 

 이런 한계를 극복하고자 XLNet은 앞뒤 문맥을 동시에 고려하는 양방향 언어 모델과 예측 단어 간 상관관계를 고려하는 순방향 모델의 장점을 합쳤다. 다만 모델이 지나치게 복잡하고, 앞서 언급한 문제가 모델 성능에 미치는 영향에 대한 명확한 근거가 제시되지 않았다는 이유로 업계 내에서는 상대적으로는 덜 사용된다고 한다. 

 RoBERTa\(A Robustly Optimized BERT\)는 BERT보다 성능을 한 단계 업그레이드한 버전이다. 모델 크기가 클수록 성능을 높일 수 있다는 판단하에 훈련 데이터의 양\(13GB→160GB\)과 학습 횟수\(125,000회→500,000회\), 배치 크기\(256→8,192\), 사전 크기\(32,000→50,000\) 등을 대폭 늘리는 전략을 취했다. BERT의 NSP 과제는 훈련에서 제외했다. 실제 의도했던 문장 간 연관 관계를 배운다기보다는, 단순히 두 문장이 같은 문맥을 따르는지를 판단하는 쪽에 가까워 학습 효과가 불분명하다고 봤기 때문이다.

ALBERT\(A Lite BERT\)는 이름 그대로 BERT보다 가벼운 모델이다. 모델 매개변수 수를 줄여 같은 구조의 모델에서의 메모리 사용량을 줄이고 학습 속도를 높였다. 우선 각 단어를 저차원의 임베딩 벡터로 먼저 표현하고 나서 이를 다시 모델 은닉층의 차원 수만큼 확장하고, 또한 Transformer 인코더 블록 간 매개변수를 공유하도록 했다. 그 결과, ALBERT\(large\)는 BERT\(large\)와 비교했을 때 매개변수 수는 1/18\(3억 3,400만개→1,800만개\)로 줄어들고 GLUE 성능을 일정 수준 유지하면서\(85.2→82.4\) 학습 속도를 1.7배 높였다. 뿐만 아니라 BERT의 NSP 대신 두 문장의 연관 관계를 예측하는 과제인 문장 순서 예측\(sentence order prediction, SOP\)을 훈련해 성능을 더 높였다.

 \(작성중\)

