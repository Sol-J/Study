{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd0f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_list(path):\n",
    "    df = pd.read_excel(path)\n",
    "    info = []\n",
    "    for idx in range(len(df)):\n",
    "        info.append(str(df['출판사 명'][idx]) + '|' + str(df['교재명'][idx]))\n",
    "    return info\n",
    "\n",
    "def open_browser():\n",
    "    options = webdriver.ChromeOptions() \n",
    "    options.add_argument('headless')\n",
    "    options.add_argument(\"disable-gpu\") \n",
    "    options.add_argument(\"disable-infobars\")\n",
    "    options.add_argument(\"--disable-extensions\")\n",
    "    prefs = {'profile.default_content_setting_values': {'cookies' : 2, 'images': 2, 'plugins' : 2, 'popups': 2, 'geolocation': 2, 'notifications' : 2, 'auto_select_certificate': 2, 'fullscreen' : 2, 'mouselock' : 2, 'mixed_script': 2, 'media_stream' : 2, 'media_stream_mic' : 2, 'media_stream_camera': 2, 'protocol_handlers' : 2, 'ppapi_broker' : 2, 'automatic_downloads': 2, 'midi_sysex' : 2, 'push_messaging' : 2, 'ssl_cert_decisions': 2, 'metro_switch_to_desktop' : 2, 'protected_media_identifier': 2, 'app_banner': 2, 'site_engagement' : 2, 'durable_storage' : 2}}   \n",
    "    options.add_experimental_option('prefs', prefs)\n",
    "    options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options) \n",
    "    return driver\n",
    "\n",
    "# 침해 기준 (침해하지 않은 사례도 걸리는 문제 고민)\n",
    "def is_infringe(publisher, book_name, content): \n",
    "    infringe = 0\n",
    "    if publisher in content:\n",
    "        infringe += 1\n",
    "\n",
    "    cnt = 0 \n",
    "    book_keywords = book_name.replace('(', ' ').replace(')', ' ').replace('  ', ' ').split(' ')\n",
    "    for noun in book_keywords:\n",
    "        if noun in content.split(' '):\n",
    "            cnt += 1\n",
    "    if cnt >= (len(book_keywords))*0.8: \n",
    "        infringe += 1\n",
    "        \n",
    "    return infringe\n",
    "\n",
    "\n",
    "# (1) 네이버 블로그\n",
    "def get_infringe_naver_blog(info):\n",
    "    publishers, book_names, inf_ids, inf_urls, inf_dates = [], [], [], [], []\n",
    "    driver = open_browser()\n",
    "    \n",
    "    for i in tqdm(range(len(info))):\n",
    "        publisher, book_name = info[i].split('|')[0], info[i].split('|')[1]\n",
    "    \n",
    "        query = f'{book_name} %26 첨부파일 %26 파일 다운로드' #f'{publisher} &26 {book_name} %26 첨부파일 %26 파일 다운로드'\n",
    "        driver.get(f'https://section.blog.naver.com/Search/Post.naver?pageNo=1&rangeType=ALL&orderBy=recentdate&keyword={query}')\n",
    "        time.sleep(0.5)\n",
    "        page_source = driver.page_source\n",
    "        bs = BeautifulSoup(page_source, 'lxml')\n",
    "\n",
    "        search_num = bs.find('em', {'class':'search_number'}).text\n",
    "        pages = math.ceil(int(search_num[:-1].replace(',', ''))/7)\n",
    "\n",
    "        links = []\n",
    "        for page in range(1, pages+1):\n",
    "            url = f'https://section.blog.naver.com/Search/Post.naver?pageNo={page}&rangeType=ALL&orderBy=recentdate&keyword={query}'\n",
    "            driver.get(url)\n",
    "            driver.implicitly_wait(3)\n",
    "            time.sleep(0.5)\n",
    "            page_source = driver.page_source\n",
    "            bs = BeautifulSoup(page_source, 'lxml')\n",
    "            post = bs.find_all('a', {'class': 'desc_inner'})\n",
    "            links = [p['href'] for p in post]\n",
    "\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            driver.switch_to.frame('mainFrame')\n",
    "            time.sleep(0.5)\n",
    "            page_source = driver.page_source\n",
    "            bs = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            if bs.find('div', {'class': 'se-module se-module-file'}) is not None:\n",
    "                content = bs.find('div', {'id':'postListBody'}).text\n",
    "                infringe = is_infringe(publisher, book_name, content)\n",
    "\n",
    "                if infringe:\n",
    "                    publishers.append(publisher)\n",
    "                    book_names.append(book_name)\n",
    "                    inf_ids.append(link.split('/')[3] + '@naver.com')\n",
    "                    inf_urls.append(link)\n",
    "                    date_tag = bs.find('span', {'class': 'se_publishDate pcol2'}).text\n",
    "                    try:\n",
    "                        date = datetime.strptime(date_tag, '%Y. %m. %d. %H:%M').strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        date = datetime.now().strftime('%Y-%m-%d')\n",
    "                    inf_dates.append(date)\n",
    "\n",
    "    return publishers, book_names, inf_ids, inf_urls, inf_dates\n",
    "\n",
    "# (2) 네이버 카페\n",
    "def get_infringe_naver_cafe(info):\n",
    "    publishers, book_names, inf_urls, inf_dates = [], [], [], []\n",
    "    driver = open_browser()\n",
    "    for i in tqdm(range(len(info))):\n",
    "        publisher, book_name = info[i].split('|')[0], info[i].split('|')[1]\n",
    "        query = f'{book_name} %26 첨부파일 %26 파일 다운로드' #f'{publisher} &26 {book_name} %26 첨부파일 %26 파일 다운로드'\n",
    "        driver.get(f'https://section.cafe.naver.com/ca-fe/home/search/articles?q={query}&od=1')\n",
    "        time.sleep(0.5)\n",
    "        page_source = driver.page_source\n",
    "        bs = BeautifulSoup(page_source, 'lxml')\n",
    "        search_num = bs.find('span', {'class': 'total_count'}).text\n",
    "        pages = math.ceil(int(search_num[:-1].replace(',', ''))/12)\n",
    "\n",
    "        links = []\n",
    "        for page in range(1, pages+1):\n",
    "            url = f'https://section.cafe.naver.com/ca-fe/home/search/articles?q={query}&p={page}&od=1'\n",
    "            driver.get(url)\n",
    "            time.sleep(0.5)\n",
    "            page_source = driver.page_source\n",
    "            bs = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            post = bs.find_all('a', {'class': 'item_subject'})\n",
    "            links = [p['href'] for p in post]\n",
    "\n",
    "        for link in links:\n",
    "            driver.get(link)\n",
    "            driver.switch_to.frame('cafe_main')\n",
    "            time.sleep(0.5)\n",
    "            page_source = driver.page_source\n",
    "            bs = BeautifulSoup(page_source, 'lxml')\n",
    "            \n",
    "            if bs.find('div', {'class': 'se-module se-module-file'}) is not None:\n",
    "                content = bs.find('div', {'class':'ArticleContentBox'}).text\n",
    "                infringe = is_infringe(publisher, book_name, content)\n",
    "\n",
    "                if infringe:\n",
    "                    publishers.append(publisher)\n",
    "                    book_names.append(book_name)\n",
    "                    inf_urls.append(link)\n",
    "                    date_tag = bs.find('span', {'class':'date'}).text\n",
    "                    try:\n",
    "                        date = datetime.strptime(date_tag, '%Y. %m. %d. %H:%M').strftime('%Y-%m-%d')\n",
    "                    except:\n",
    "                        date = datetime.now().strftime('%Y-%m-%d')\n",
    "                    inf_dates.append(date) \n",
    "                    \n",
    "        print(publishers, book_names, inf_urls, inf_dates)\n",
    "\n",
    "    return publishers, book_names, inf_urls, inf_dates\n",
    "\n",
    "# (3) 다음 블로그 (티스토리)\n",
    "def tistory_link_loc(link):\n",
    "    hdr = {'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64)' \n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36')}\n",
    "    response = requests.get(link, headers = hdr) \n",
    "    bs = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    if bs.find('figure', {'class':'fileblock'}) is not None:\n",
    "        content = bs.find('div', {'class':'contents_style'}).text\n",
    "        infringe = is_infringe(publisher, book_name, content)\n",
    "        if infringe:\n",
    "            publishers.append(publisher)\n",
    "            book_names.append(book_name)\n",
    "            inf_ids.append(link.split('/')[2].split('.')[0])\n",
    "            inf_urls.append(link)\n",
    "            try:\n",
    "                date_tag = bs.select_one('span.date').text\n",
    "                date = datetime.strptime(date_tag, '%Y. %m. %d. %H:%M').strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                date = datetime.now().strftime('%Y-%m-%d')\n",
    "            inf_dates.append(date) \n",
    "            \n",
    "def get_infringe_tistory_blog(info):\n",
    "    \n",
    "    global publishers, book_names, inf_ids, inf_urls, inf_dates \n",
    "    publishers, book_names, inf_ids, inf_urls, inf_dates = [], [], [], [], []\n",
    "    global publisher, book_name\n",
    "    \n",
    "    for i in tqdm(range(len(info))):\n",
    "        publisher, book_name = info[i].split('|')[0], info[i].split('|')[1]\n",
    "    \n",
    "        query = f'{book_name} %26 pdf'\n",
    "        url = f'https://search.daum.net/search?w=blog&f=section&SA=tistory&lpp=10&nil_src=tistory&q={query}&sort=timely'\n",
    "        hdr = {'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64)' \n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36')}\n",
    "        response = requests.get(url, headers = hdr) \n",
    "        bs = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        try: \n",
    "            search_num = bs.find('span', {'class':'txt_info'}).text.split(' / ')[1]\n",
    "            pages = math.ceil(int(search_num[:-1].replace(',', ''))/10)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        for page in range(1, pages+1):\n",
    "            url = f'https://search.daum.net/search?w=blog&f=section&SA=tistory&lpp=10&nil_src=tistory&q={query}&p={page}&sort=timely'\n",
    "            response = requests.get(url, headers = hdr) \n",
    "            bs = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            post = bs.find_all('a', {'class':'f_link_b'})\n",
    "            links = [p['href'] for p in post]\n",
    "        \n",
    "        links = [link for link in links if 'tistory' in link]\n",
    "        for link in links:\n",
    "            tistory_link_loc(link)\n",
    "\n",
    "    return publishers, book_names, inf_ids, inf_urls, inf_dates\n",
    "\n",
    "# (4) 다음 카페\n",
    "def daum_cafe_link_loc(link):\n",
    "    driver.get(link)\n",
    "    driver.switch_to.frame('down')\n",
    "    page_source = driver.page_source\n",
    "    bs = BeautifulSoup(page_source, 'lxml')\n",
    "    \n",
    "    if bs.find('div', {'class':'AFArea'}) is not None:\n",
    "        content = bs.find('div', {'class':'bbs_contents'}).text\n",
    "        infringe = is_infringe(publisher, book_name, content)\n",
    "        if infringe:\n",
    "            publishers.append(publisher)\n",
    "            book_names.append(book_name)\n",
    "            inf_urls.append(link)\n",
    "            try:\n",
    "                date_tag = bs.find_all('span', {'class': 'txt_item'})[2].text\n",
    "                date = datetime.strptime(date_tag, '%y.%m.%d %H:%M').strftime('%Y-%m-%d')\n",
    "            except:\n",
    "                date = datetime.now().strftime('%Y-%m-%d')\n",
    "            inf_dates.append(date) \n",
    "            \n",
    "def get_infringe_daum_cafe(info):\n",
    "    \n",
    "    global publishers, book_names, inf_urls, inf_dates \n",
    "    publishers, book_names, inf_urls, inf_dates = [], [], [], []\n",
    "    global publisher, book_name\n",
    "    global driver\n",
    "    driver = open_browser()\n",
    "    \n",
    "    for i in tqdm(range(len(info))):\n",
    "        publisher, book_name = info[i].split('|')[0], info[i].split('|')[1]\n",
    "    \n",
    "        query = f'{book_name} %26 pdf'\n",
    "        url = f'https://top.cafe.daum.net/_c21_/search?search_opt=board&SearchType=tab&sort_type=recency&q={query}&p=1'\n",
    "        hdr = {'user-agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64)' \n",
    "                      'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36')}\n",
    "        response = requests.get(url, headers = hdr) \n",
    "        bs = BeautifulSoup(response.text, 'lxml')\n",
    "        \n",
    "        try: \n",
    "            search_num = re.findall(r'\\d+', bs.find('span', {'class':'expander_scafe'}).text.replace(',', '').split('/')[-1])\n",
    "            pages = math.ceil(int(search_num[0])/10)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "        for page in range(1, pages+1):\n",
    "            url = f'https://top.cafe.daum.net/_c21_/search?search_opt=board&SearchType=tab&sort_type=recency&q={query}&p={page}'\n",
    "            response = requests.get(url, headers = hdr) \n",
    "            bs = BeautifulSoup(response.text, 'lxml')\n",
    "            \n",
    "            post = bs.find_all('a', {'class':'link_tit'})\n",
    "            links = [p['href'] for p in post]\n",
    "        \n",
    "        for link in links:\n",
    "            daum_cafe_link_loc(link)\n",
    "\n",
    "    return publishers, book_names, inf_urls, inf_dates\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import selenium\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    from selenium.webdriver.remote.webelement import WebElement\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    import time\n",
    "    from datetime import datetime\n",
    "    import multiprocessing\n",
    "    from multiprocessing import Pool\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    import csv\n",
    "    import math\n",
    "    from tqdm.auto import tqdm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
